{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykład 12: Projektowanie sieci ConvNet\n",
    "\n",
    "Na tym wykładzie zaprojektujemy sami sieci ConvNet i prostymi zastosowaniami\n",
    "\n",
    "Zobaczymy także trzy różne metody definiowania modeli w Kerasie:\n",
    "\n",
    "- Sekewncyjny (Sequential)\n",
    "- Funcjonalny (Functional)\n",
    "- Zorientowany obietkowo (Object-Oriented)\n",
    "\n",
    "Uwaga: Celem tego wykładu nie jest porównanie wydajności, ale tylko samych architektur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytywanie danych (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "h, w = x_train.shape[1:]\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], h, w, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], h, w, 1)\n",
    "input_shape = (h, w, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=10000, random_state=42)\n",
    "\n",
    "(x_train.shape, y_train.shape), (x_val.shape, y_val.shape), (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[0].squeeze(-1))\n",
    "plt.title(y_train[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"{} unique labels.\".format(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LeNet\n",
    "\n",
    "Na początku zdefiniujemy klasyczny model LeNet-5 wprowadzony przez Yann Le Cun in 1998 ([url](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)). Model jest na tyle prosty, że możemy go łatwo zdefiniować z wykorzystaniem **Sequential** API.\n",
    "\n",
    "![lenet archi](images/lenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AvgPool2D, MaxPool2D, Dense, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "lenet = Sequential(name=\"LeNet-5\")\n",
    "\n",
    "lenet.add(Conv2D(6, kernel_size=(5, 5), activation=\"tanh\", padding=\"same\", input_shape=input_shape, name=\"C1\"))\n",
    "lenet.add(MaxPool2D(pool_size=(2, 2), name=\"S2\"))\n",
    "lenet.add(Conv2D(16, kernel_size=(5, 5), activation='tanh', name=\"C3\"))\n",
    "lenet.add(AvgPool2D(pool_size=(2, 2), name=\"S4\"))\n",
    "lenet.add(Conv2D(120, kernel_size=(5, 5), activation='tanh', name=\"C5\"))\n",
    "lenet.add(Flatten())\n",
    "lenet.add(Dense(84, activation='tanh', name=\"F6\"))\n",
    "lenet.add(Dense(10, activation='softmax'))\n",
    "\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "lenet.compile(\n",
    "    optimizer=optimizers.SGD(lr=0.1),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "lenet.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga: LeNet został na początku zdefiniowany z wykorzystaniem funkcji aktywacji `tanh` lub `sigmoid`, aktualnie\n",
    "te funkcje aktywacji są rzadko używane. Można pokazać, że obie funkcje \"nasycają się\" (saturate)\n",
    "dla bardzo małych i bardzo dużych wartości co powoduje \"zanik\" gradientu.\n",
    "\n",
    "Dlatego aktualnie większość sieci wykorzystuje `ReLU` w warstwach ukrytych lub inne o podobnych własnościach\n",
    "(https://keras.io/layers/advanced-activations/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inception\n",
    "\n",
    "Model \"Inception\" został wprowadzony w 2014 przez Szegedy et al. ([paper url](https://arxiv.org/abs/1409.4842)).\n",
    "\n",
    "Konwolucje mają efektywne \"pole chłonności\" (receptive field): im większe \"kernels\" i im głębszy (deeper) model, tym więcej pikseli obrazu *zobaczymy*. Dobrze wyjaśnienia to: [medium blog](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807).\n",
    "\n",
    "W modelu tym, różne \"convolution kernels\" są ze sobą łączone. Mniejsze \"kernels\" wykorzystywane są do mniejszych\n",
    "klastrów cech (można myśleć o tym jak o detalach np. oko), podchas duże \"kernels\" dla większych kastrów cech\n",
    "(np. twarz)\n",
    "\n",
    "![inception archi](images/inception.png)\n",
    "\n",
    "Tym razem wykorzystamy **Functional** API  do zdefiniowania pojedynczej warstwy \"Inception layer\"\n",
    "\n",
    "Przykład:\n",
    "\n",
    "```python\n",
    "a = Input(shape=(32,))\n",
    "b = Dense(32)(a)\n",
    "model = Model(inputs=a, outputs=b)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def inception_layer(tensor, n_filters):\n",
    "    branch1x1 = Conv2D(n_filters, kernel_size=(1, 1), activation=\"relu\", padding=\"same\")(tensor)\n",
    "    branch5x5 = Conv2D(n_filters, kernel_size=(5, 5), activation=\"relu\", padding=\"same\")(tensor)\n",
    "    branch3x3 = Conv2D(n_filters, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(tensor)\n",
    "\n",
    "    branch_pool = MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding=\"same\")(tensor)\n",
    "\n",
    "    output = Concatenate(axis=-1)(\n",
    "        [branch1x1, branch5x5, branch3x3, branch_pool]\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "input_tensor = Input(shape=input_shape)\n",
    "x = Conv2D(16, kernel_size=(5, 5), padding=\"same\")(input_tensor)\n",
    "x = inception_layer(x, 32)\n",
    "x = Flatten()(x)\n",
    "output_tensor = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "mini_inception = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "mini_inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "mini_inception.compile(\n",
    "    optimizer=optimizers.SGD(lr=0.1),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "mini_inception.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_inception.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ResNet\n",
    "\n",
    "ResNet (*Residual Networks*) model został wprowadzony przez He et al. in 2015 ([paper url](https://arxiv.org/abs/1512.03385)). Autorzy pracy zauważyli, że więcej warstw znacznie poprawia wydajność sieci, niestety\n",
    "bardzo trudno uczyć takie sieci (backpropagate the gradients).\n",
    "\n",
    "Trik który pozwalał uczyć takie sieci umożliwiając łatwiejszy \"przepływ\" gradientu polega na wprowadzeniu skrótów\n",
    "\n",
    "![resnet archi](images/resnet.png)\n",
    "\n",
    "Tym razem wykorzystamy **Oriented-Object** API:\n",
    "\n",
    "Przykład:\n",
    "```python\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        self.classifier = Dense(10, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.classifier(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Add, Layer, Activation\n",
    "\n",
    "\n",
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__(name=\"ResidualBlock\")\n",
    "\n",
    "        self.conv1 = Conv2D(n_filters, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")\n",
    "        self.conv2 = Conv2D(n_filters, kernel_size=(3, 3), padding=\"same\")\n",
    "        self.add = Add()\n",
    "        self.last_relu = Activation(\"relu\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(inputs)\n",
    "\n",
    "        y = self.add([x, inputs])\n",
    "        y = self.last_relu(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class MiniResNet(Model):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = Conv2D(n_filters, kernel_size=(5, 5), padding=\"same\")\n",
    "        self.block = ResidualBlock(n_filters)\n",
    "        self.flatten = Flatten()\n",
    "        self.classifier = Dense(10, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.block(x)\n",
    "        x = self.flatten(x)\n",
    "        y = self.classifier(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "mini_resnet = MiniResNet(32)\n",
    "mini_resnet.build((None, *input_shape))\n",
    "mini_resnet.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "mini_resnet.compile(\n",
    "    optimizer=optimizers.SGD(lr=0.1),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "mini_resnet.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_resnet.evaluate(x_test, y_test, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
