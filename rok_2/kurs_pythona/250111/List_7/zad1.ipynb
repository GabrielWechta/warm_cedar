{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"zad1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPryD+0dIEnSosN0tHLa1v3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3Las6T2kXq2C","colab_type":"text"},"source":["**Zadanie 1, Lista 7**\n","\n","Przy użyciu kerasa, nauczymy sieć neuronową przewdidywać wartości funkcji (x^2, sin(x)).\n","\n","Zaproponuję trzy modele, które później przetestujemy na funkcjach. Modele będą się różnić, co da nam różne wyniki i pozwoli porównać architekturę modeli.\n","\n","Żeby w wyścigu po najlepszą sieć były równe szanse, gęstość próbkowania training_setu będzie stała, liczba epok również.\n","\n","*Na marginesie chciałbym dodać, że moim zdaniem, w każdym mieście na świecie powinien stać pomnik poświęcony ludziom odpowiedzialnym za coolaba, albo chociaż jakiś skwer.*"]},{"cell_type":"code","metadata":{"id":"n6_pbAuY7E0P","colab_type":"code","outputId":"ed8eb488-a2f5-4f18-f750-3a579ae2fb12","executionInfo":{"status":"ok","timestamp":1591904858724,"user_tz":-120,"elapsed":5017,"user":{"displayName":"Gabriel Wechta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6RI_R3jryiV_baAo4PkmgTVEv-mCk7XdyV5s=s64","userId":"11586536003816201804"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%reset"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pVsrAArneCE4","colab_type":"text"},"source":["**Funkcja kwadratowa**\n","\n","\n","*   Podniosłem próbkowanie do 10000.\n","*   40 epok wystarcza na elegenckie wytrenowanie.\n","\n"]},{"cell_type":"code","metadata":{"id":"eRLdAiQFOeAd","colab_type":"code","colab":{}},"source":["import numpy as np\n","from sklearn import preprocessing\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation\n","from tensorflow.keras import optimizers\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","\n","x_train = np.linspace(-50,50,10000)\n","y_train = x_train**2\n","x_train=x_train.reshape(len(x_train),1)\n","\n","x_test = np.linspace(-50,50,101)\n","y_test = x_test**2\n","x_test=x_test.reshape(len(x_test),1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LI3w7oPLZmJ9","colab_type":"text"},"source":["Pierwszy model:\n","\n","\n","*   jedna wartswa ukryta (odpowiednik 1-10-1 zadania z listy 6)\n","*   activation = relu\n","*   loss = mean_squared_error\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"aNTJNrnBQPOo","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Dense(units=10, input_dim=1))\n","model.add(Activation('relu'))\n","model.add(Dense(units=1))\n","model.compile(loss='mean_squared_error',\n","              optimizer='adam')\n","\n","model.fit(x_train, y_train, epochs=40, batch_size=50, verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wsGXflmrekV","colab_type":"code","colab":{}},"source":["pred = model.predict(x_test, batch_size=1)\n","\n","test=x_test.reshape(-1)\n","plt.plot(test,pred,c='r')\n","plt.plot(test,y_test,c='b')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4MSc6MFMr_6j","colab_type":"text"},"source":["Wyszło trochę słabo. Uzyskany wynik przypomina raczej f(x) = |x|.\n","\n","Kolejny model ma więcej, bogatszych wartsw, co daje dużo lepsze wyniki."]},{"cell_type":"markdown","metadata":{"id":"ZPR5cEjdsi5L","colab_type":"text"},"source":["Drugi model:\n","\n","*   dwie wartswy ukryte\n","*   activation = relu\n","*   loss = mean_squared_error\n"]},{"cell_type":"code","metadata":{"id":"7SgHxS30abM6","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Dense(units=200, input_dim=1))\n","model.add(Activation('relu'))\n","model.add(Dense(units=45))\n","model.add(Activation('relu'))\n","model.add(Dense(units=1))\n","\n","model.compile(loss='mean_squared_error',\n","              optimizer='adam')\n","\n","model.fit(x_train, y_train, epochs=40, batch_size=50, verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTr3AK7wf6ZC","colab_type":"code","colab":{}},"source":["pred = model.predict(x_test, batch_size=1)\n","\n","test=x_test.reshape(-1)\n","plt.plot(test,pred,c='r')\n","plt.plot(test,y_test,c='b')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5qWN4X49vHBT","colab_type":"text"},"source":["Trzeci model:\n","\n","*   Zmieniłem learning_rate Adama na 0.005 (deafult 0.001). Spowodowało to chaotyczne zachowanie się loss. Eksperymentalnie sprawdziłem, że czasami szybciej znajdowało lepsze wyniki niż model drugi, natomiast od raz z tych optimów wypadało.\n","*   activation = selu (Scaled Exponential Linear Unit)"]},{"cell_type":"code","metadata":{"id":"60N-QbM2s3Fv","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Dense(units=200, input_dim=1, kernel_initializer='lecun_normal'))\n","model.add(Activation('selu'))\n","model.add(Dense(units=45, kernel_initializer='lecun_normal'))\n","model.add(Activation('selu'))\n","model.add(Dense(units=1))\n","\n","model.compile(loss='mean_squared_error',\n","              optimizer=optimizers.Adam(learning_rate=0.005))\n","\n","model.fit(x_train, y_train, epochs=40, batch_size=50, verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WiDP0_9ttSHz","colab":{}},"source":["pred = model.predict(x_test, batch_size=1)\n","\n","test=x_test.reshape(-1)\n","plt.plot(test,pred,c='r')\n","plt.plot(test,y_test,c='b')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ha-kjQ3UwaL_","colab_type":"text"},"source":["**Funkcja sinus**\n","\n","\n","*   Podniosłem próbkowanie do 5000.\n","*   Wziąłęm również większą dziedzinę, zobaczymy jak nn sobie z tym poradzi.\n","*   Pondiosłem liczbę epok, aby dać jej szanszę pojąć okresowość sinusa.\n"]},{"cell_type":"code","metadata":{"id":"T9ySQjW0wvop","colab_type":"code","colab":{}},"source":["x_train = np.linspace(0,4,5000)\n","y_train = np.sin((3*np.pi/2) * x_train)\n","x_train=x_train.reshape(len(x_train),1)\n","\n","x_test = np.linspace(0,4,101)\n","y_test = np.sin((3*np.pi/2) * x_test)\n","x_test=x_test.reshape(len(x_test),1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCDUVl5iy8oS","colab_type":"text"},"source":["Pierwszy model:\n","\n","Dla porównania znów weżmiemy czysty 1-10-1.\n","\n","Jak widzimy, model wygenerował piękną prostą, natomaist słabo przybliżył sinus."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"clHi35SfyUoT","colab":{}},"source":["model = Sequential()\n","model.add(Dense(units=10, input_dim=1))\n","model.add(Activation('relu'))\n","model.add(Dense(units=1))\n","model.compile(loss='mean_squared_error',\n","              optimizer='adam')\n","\n","model.fit(x_train, y_train, epochs=40, batch_size=50, verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"duZoxDhryav9","colab_type":"code","colab":{}},"source":["pred = model.predict(x_test, batch_size=1)\n","\n","test=x_test.reshape(-1)\n","plt.plot(test,pred,c='r')\n","plt.plot(test,y_test,c='b')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kTYxxWPIzM4q","colab_type":"text"},"source":["Drugi model:\n","\n","W tym modelu, dostajemy już dobre przybliżenie, zwłascza dla początkowych wartości."]},{"cell_type":"code","metadata":{"id":"BcDWZm5wzQFE","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Dense(units=200, input_dim=1))\n","model.add(Activation('relu'))\n","model.add(Dense(units=45))\n","model.add(Activation('relu'))\n","model.add(Dense(units=1))\n","\n","model.compile(loss='mean_squared_error',\n","              optimizer='adam')\n","\n","model.fit(x_train, y_train, epochs=40, batch_size=50, verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nh8_k_DUzUQR","colab_type":"code","colab":{}},"source":["pred = model.predict(x_test, batch_size=1)\n","\n","test=x_test.reshape(-1)\n","plt.plot(test,pred,c='r')\n","plt.plot(test,y_test,c='b')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"90zhouTMzofv","colab_type":"text"},"source":["Trzeci model:\n","\n","Trochę gorszy niż poprzedni."]},{"cell_type":"code","metadata":{"id":"ylpXGXJ3zu3W","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Dense(units=200, input_dim=1, kernel_initializer='lecun_normal'))\n","model.add(Activation('selu'))\n","model.add(Dense(units=45, kernel_initializer='lecun_normal'))\n","model.add(Activation('selu'))\n","model.add(Dense(units=1))\n","\n","model.compile(loss='mean_squared_error',\n","              optimizer=optimizers.Adam(learning_rate=0.005))\n","\n","model.fit(x_train, y_train, epochs=40, batch_size=50, verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMmM3tJJzx4T","colab_type":"code","colab":{}},"source":["pred = model.predict(x_test, batch_size=1)\n","\n","test=x_test.reshape(-1)\n","plt.plot(test,pred,c='r')\n","plt.plot(test,y_test,c='b')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GXVkNvo5z_zY","colab_type":"text"},"source":["Sprawdźmy jeszcze większą dziedzinę dla drugiego modelu.\n","\n","Zwiększymy również liczbę epok."]},{"cell_type":"code","metadata":{"id":"JFvkelAf0R0A","colab_type":"code","colab":{}},"source":["x_train = np.linspace(0,8,10000)\n","y_train = np.sin((3*np.pi/2) * x_train)\n","x_train=x_train.reshape(len(x_train),1)\n","\n","x_test = np.linspace(0,8,1001)\n","y_test = np.sin((3*np.pi/2) * x_test)\n","x_test=x_test.reshape(len(x_test),1)\n","\n","model = Sequential()\n","model.add(Dense(units=200, input_dim=1))\n","model.add(Activation('relu'))\n","model.add(Dense(units=45))\n","model.add(Activation('relu'))\n","model.add(Dense(units=1))\n","\n","model.compile(loss='mean_squared_error',\n","              optimizer='adam')\n","\n","model.fit(x_train, y_train, epochs=100, batch_size=50, verbose=1)\n","\n","pred = model.predict(x_test, batch_size=1)\n","\n","test=x_test.reshape(-1)\n","plt.plot(test,pred,c='r')\n","plt.plot(test,y_test,c='b')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7wcVFgeH3yMU"},"source":["Jak widać, dla większego zakresu, radzi sobie słabo. Daje dobre przyblizenie dla początkowych wartości, ale później zupełnie się gubi.\n","\n"]},{"cell_type":"code","metadata":{"id":"Oc3hQ6t43kne","colab_type":"code","colab":{}},"source":["x_train = np.linspace(0,8,10000)\n","y_train = np.sin((3*np.pi/2) * x_train)\n","x_train=x_train.reshape(len(x_train),1)\n","\n","x_test = np.linspace(0,8,1001)\n","y_test = np.sin((3*np.pi/2) * x_test)\n","x_test=x_test.reshape(len(x_test),1)\n","\n","model = Sequential()\n","model.add(Dense(units=200, input_dim=1))\n","model.add(Activation('relu'))\n","model.add(Dense(units=105))\n","model.add(Activation('relu'))\n","model.add(Dense(units=45))\n","model.add(Activation('relu'))\n","model.add(Dense(units=1))\n","\n","model.compile(loss='mean_squared_error',\n","              optimizer='adam')\n","\n","model.fit(x_train, y_train, epochs=100, batch_size=50, verbose=1)\n","\n","pred = model.predict(x_test, batch_size=1)\n","\n","test=x_test.reshape(-1)\n","plt.plot(test,pred,c='r')\n","plt.plot(test,y_test,c='b')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T88XoPhg1wCb","colab_type":"text"},"source":["Dodanie dodatkowej warstwy znacznie porpawiło wyniki.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OIDPAHNq3fCx","colab_type":"text"},"source":["**Podsumowanie**\n","\n","Używałem funckji aktywacji, które nadają się do formatu danych jakich używałem. Zarówno relu i selu dawały dobre wyniki, przy czym dla selu nalezało ustawić inny kernel_initializer. \n","\n","Zauważyłem, że dodawanie wartw poprawia zdolności predykcyjne sieci (a wręcz je umożliwia)\n","\n","Również nalezy być ostrożnym z learning_rate, za małe daje za wolne szukanie minimum, zaś za duże wypada z minimum."]}]}